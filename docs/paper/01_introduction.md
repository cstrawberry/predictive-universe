# 1 Introduction

**1.1 Background and Motivation**

Any attempt to construct a fundamental theory of reality confronts a profound initial choice: where to begin? Modern physics typically presupposes an objective, external world of matter and energy, governed by mathematical laws, from which phenomena like consciousness are presumed to emerge. This approach, however, begins with an axiom—the existence of a mind-independent reality—that is powerful yet fundamentally unprovable. It leaves us with the intractable "hard problem" of explaining how subjective experience could arise from non-conscious constituents.

This paper proposes a different, more rigorous starting point, one grounded in what can be considered the most robust epistemological starting point we possess. Following René Descartes' foundational insight [Descartes 1641], we recognize that while all external perceptions can be doubted, the act of doubting itself proves the undeniable existence of *a process*. This self-verifying loop is not an assumption but a logical necessity. From this logical starting point, our core methodological objective becomes clear: to construct the most powerful and efficient model for predicting the behavior of this process and the reality it appears to inhabit.

The central challenge of this work is therefore to determine which model of reality provides the greatest predictive power for the least complexity. While prediction is not the only conceivable model for this foundational process, it is uniquely generative because it logically subsumes other core concepts: computation is required to generate predictions, interaction is required for verification, and distinction-making is required to differentiate states. This leads to the core axiomatic interpretation of the Predictive Universe (PU) framework: that the operational essence of the foundational process is *prediction*—a continuous, adaptive cycle of modeling, anticipating, and verifying experience. This is not presented as a logical deduction, but as the foundational modeling choice upon which this physical theory is built, defended on the grounds of its unique generative power and its ability to provide a computable, operational basis for a theory of a knowable reality.

Evaluating reality models through the lens of predictive efficiency leads us to a crucial choice. A model positing a single, monolithic predictor simulating the entire cosmos is one possibility, but it carries an immense, arguably infinite, complexity cost. A far more efficient model, we argue, is one where reality emerges from a network of simple, interacting predictive units governed by a few universal rules. This latter model offers vastly greater explanatory compression and is thus strongly favored by Occam's Razor.

Therefore, this framework adopts the network model not as an ontological assumption about an external world, but as the *deductively chosen optimal predictive framework*. From this operational and non-arbitrary starting point, the framework seeks to derive the laws of physics not as pre-existing rules, but as the necessary emergent structures and constraints governing any such self-consistent, resource-limited, predictive reality. Consciousness is therefore not a late-stage emergent property to be explained, but is related to the foundational process whose operational logic dictates the very structure of the physical world. The challenge is to demonstrate that the entire edifice of modern physics—including quantum mechanics, spacetime geometry, and the arrow of time—can be reconstructed as the necessary consequence of a universe best modeled by the logic of distributed, efficient prediction. A note on methodology: The framework proceeds by positing foundational principles (e.g., POP, PCE) and constructing a formal mathematical model to represent them. The 'Theorems' presented are rigorous proofs of the necessary consequences *within this model*. Their applicability to physical reality therefore hinges on the validity of the foundational principles and the accuracy of the model. For clarity, key terms are defined in the Glossary.

This approach, which derives physical dynamics from the logic and economics of prediction itself, engages with and offers a distinct perspective on several active areas of research. Insights from cognitive science, particularly predictive processing theories [Clark 2013; Friston 2010; Hohwy 2016; Seth 2021], alongside complexity science [Anderson 1972; Gell-Mann 1994] and information theory [Shannon 1948; Wheeler 1990], increasingly suggest that prediction and optimization under constraints are crucial organizing principles across multiple scales. Concurrently, information-theoretic concepts are informing investigations into the possible emergence of spacetime geometry from more fundamental principles [Jacobson 1995; Verlinde 2011; Van Raamsdonk 2010].

However, a critical gap persists that the PU framework aims to fill: physical theories typically treat the resources required for computation and prediction—such as complexity, energy, and time—as external parameters rather than deriving their interplay from first principles. Conversely, abstract information-theoretic models often lack explicit grounding in physical dynamics and the resource limitations imposed by thermodynamics and causality.

By grounding its axioms in the operational cycle of prediction, the PU framework also offers a novel perspective on long-standing debates in the interpretation of quantum mechanics. Unlike theories treating consciousness merely as an emergent property of complex systems [Searle 1992], or proposals directly linking macroscopic consciousness to quantum measurement [Wigner 1967; von Neumann 1955], the PU framework posits a universal actualization mechanism—the 'Evolve' process (Definition 27)—which is intrinsically linked to the operational cycle of its fundamental constituents. A subsequent conjecture about 'Minimal Awareness' (Postulate 1) is then introduced based on a principle of symmetry, suggesting a potential experiential counterpart to this fundamental process. The Consciousness Complexity (CC) hypothesis (Section 9) then explores how more complex systems might modulate this process. This hierarchical view distinguishes it from other interpretations like QBism [Fuchs, Mermin & Schack 2014], the Many-Worlds Interpretation [Everett 1957], or Relational Quantum Mechanics [Rovelli 1996], none of which provide the same specific integration of prediction, resource cost, and emergent dynamics central to this work.

**1.2 Overview of the Framework**

The Predictive Universe framework unfolds from foundational principles governing prediction and resource optimization. It establishes the *Prediction Optimization Problem* (POP, Axiom 1) as the core adaptive imperative and defines information functionally (Definition 1) relative to this goal (Axiom 2). The crucial concept of *Predictive Physical Complexity* ($C_P$) quantifies the minimal resources needed for predictive capability (Equation 1), linked dynamically to an operational proxy $\hat{C}_v$ (Theorem 1, Theorem 2). Resource cost functions ($R, R_I$, Definition 3) and corresponding operators (Theorem 3) capture the physical price of complexity.

The core operational cycle, the *Fundamental Predictive Loop* (Definition 4), encapsulates the adaptive process. Interactions within this loop are formalized by *Reflexive Interaction Dynamics* (RID, Definition 6), highlighting inherent feedback structures. The necessity of operating within specific performance bounds—the *Space of Becoming* $(\alpha, \beta)$ (Definition 8)—is derived (Theorem 8, Theorem 9), establishing operational viability (Axiom 3).

Analysis of self-reference within computationally rich systems (Property R, Definition 10) leads to the *Self-Referential Paradox of Accurate Prediction* (SPAP, Theorem 10, Theorem 11) and *Reflexive Undecidability* (Theorem 12), proving fundamental *Logical Indeterminacy* (Definition 12). This motivates defining the *Operational Threshold* ($C_{op}$, Definition 13) as the minimum complexity for the adaptive loop, and identifying the distinct *Horizon Constant* ($K_0$, Theorem 15) as the minimum complexity to instantiate the internal logic of self-reference (SPAP) *and* achieve predictive accuracy strictly better than chance, with $C_{op} \ge K_0$ (Corollary 3).

Adaptation dynamics are governed by the *Principle of Compression Efficiency* (PCE, Definition 15), driving systems to optimize performance under resource constraints via the *Adaptation Driving Force* ($\Psi$, Definition 20). This yields the *Law of Prediction* (Theorem 19), relating complexity to performance within the viable range $(\alpha, \beta)$.

The framework's core entities, *Minimal Predictive Units* (MPUs, Definition 23), are introduced via Hypothesis 1 as systems operating at the $C_{op}$ threshold. Their state is the *Perspectival State* ($S_{(s)}(t)$, Definition 24) within an emergent complex Hilbert space $\mathcal{H}_0$ (Proposition 4). MPUs follow *Dual Dynamics*: deterministic, unitary *Internal Prediction* (Definition 26, Schrödinger equation) and stochastic *'Evolve'* interactions (Definition 27, ND-RID). The 'Evolve' process is constrained by fundamental thermodynamic costs ($\varepsilon \ge \ln 2$, Theorem 31; Reflexivity Constraint $\kappa_r > 0$, Theorem 33), with the fundamental irreversibility of this process ($\varepsilon$-cost) also providing a microscopic physical enforcement mechanism for the emergent arrow of time (Appendix O, Theorem O.3).

The formalism of quantum mechanics emerges as the necessary effective description of MPU dynamics (Section 8), including the *Born rule* (Proposition 7) derived from consistency principles. Measurement is explained as perspectival actualization via the 'Evolve' process.

The *Consciousness Complexity* (CC) hypothesis (Hypothesis 3) proposes that complex MPU aggregates ($C_{agg} > C_{op}$) develop an emergent capability (Theorem 34) to subtly bias 'Evolve' probabilities (Definition 30), constrained by causality ($\alpha_{CC,max} < 0.5$, Theorem 39) according to the framework's definition (Postulate 2). This allows for potential statistical faster-than-light influence (Postulate 3), analyzed for consistency.

Conditional on the necessary emergence of geometric regularity (Theorem 43, justified via Appendices C, D), Lorentzian spacetime geometry ($g_{\mu\nu}$, Theorem 46) and Einstein's Field Equations (Theorem 50) are derived thermodynamically from MPU network dynamics, utilizing the *Horizon Entropy Area Law* (Theorem 49, derived in Appendix E) and the *MPU Stress-Energy Tensor* ($T_{\mu\nu}^{(MPU)}$, Definition B.8, Appendix B) acting as the source. The framework culminates in proposals for specific experimental tests designed to probe its novel predictions, particularly the CC hypothesis.

**1.3 Key Contributions**

The primary contributions of this paper encompass the development and presentation of the Predictive Universe framework itself, including:

*   A foundational structure derived from operational principles of prediction, optimization (POP, Axiom 1; PCE, Definition 15), and resource constraints (Predictive Physical Complexity $C_P$, Equation 1; resource costs $R, R_I$, Definition 3).
*   A formal operational model linking aspects of consciousness to the adaptive Fundamental Predictive Loop (Definition 4) operating within derived viability bounds ($\alpha, \beta$, Definition 8).
*   A rigorous formal proof of the Self-Referential Paradox of Accurate Prediction (SPAP, Theorem 10, Theorem 11) under explicitly stated conditions (Property R, Definition 10), establishing fundamental Logical Indeterminacy (Definition 12) inherent in self-referential systems.
*   The identification of the Horizon Constant ($K_0$, Theorem 15) as the fundamental minimum Predictive Physical Complexity ($C_P = 3$ bits) required to instantiate the *internal logic* of self-reference (SPAP) *and* achieve predictive accuracy strictly better than chance.
*   The definition of the Operational Threshold ($C_{op}$, Definition 13) as the minimum $C_P$ required for a specific adaptive Fundamental Predictive Loop implementation to achieve a target accuracy $\epsilon_{acc}$ (Definition 13) significantly better than chance, necessarily satisfying $C_{op} \ge K_0 = 3$ bits (Corollary 3).
*   Justification for the operational complexity proxy $\hat{C}_v$ (Theorem 1) via dynamically enforced alignment with $C_P$ at equilibrium (Theorem 2, Appendix D).
*   The Minimal Predictive Unit (MPU) model (Hypothesis 1), proposing fundamental predictive constituents (Definition 23) embodying the $K_0$ minimal predictor capability.
*   A derivational pathway for the quantum mechanical formalism as the necessary, PCE-optimal description of MPU dynamics, culminating in a complete, first-principles computational program for determining the value of the electromagnetic fine-structure constant, $\alpha_{em}$ (**Appendix G.9**). The value emerges as the unique, stable equilibrium of a **rate-level PCE potential** that balances the thermodynamic power cost of maintaining U(1) coherence against the predictive information rate benefit it provides, reducing the calculation to the evaluation of operational functionals of the MPU's fundamental cycle. This derivation also provides robust, testable bounds—and, at a symmetric reference point, a parameter-free identity—linking fundamental constants to information-theoretic invariants of the MPU cycle (**Appendix W**).
*   A first-principles derivation of the ratio between the fundamental MPU spacing and the emergent Planck length (`δ/L_P`) from PCE optimization, yielding a parameter-free `O(1)` constant from the MPU's core information-theoretic parameters (`d₀=8`, `ε=ln(2)`) (**Appendix Q**).
*   A non-perturbative derivation of the cosmological constant `Λ` from the MPU vacuum dynamics, explaining its small observed value as an exponential suppression by an action fixed by the PCE-optimal information ratio `C_max/ε = 2` (**Appendix U**), leading to a computable program to invert for the underlying instanton complexity `κ` (**Appendix V**). This derivation also provides robust, testable bounds—and, at a symmetric reference point, a parameter-free identity—linking fundamental constants to information-theoretic invariants of the MPU cycle (**Appendix W**).
*   The Consciousness Complexity (CC) hypothesis (Hypothesis 3), proposing a specific, physically bounded mechanism ($\text{CC} < 0.5$, **Theorem 39**) for complex predictive systems to influence quantum outcomes (Definition 30), alongside a specific stance on causality (Postulate 2) and potential statistical FTL influence (Postulate 3).
*   A conditional derivation of emergent Lorentzian spacetime geometry (Section 11) and Einstein's Field Equations (Theorem 50) from the causal structure and thermodynamic consistency requirements of the MPU network, explicitly linked to the necessary emergence of geometric regularity (Theorem 43) and fundamental information-processing limits (Horizon Entropy Area Law, Theorem 49 derived from ND-RID irreversibility ($\varepsilon \ge \ln 2$) and channel contractivity (Appendix E)), with the MPU Stress-Energy Tensor (Definition B.8, Appendix B) acting as the source.
*   A unification of predictive and relativistic limits via the **Unified Cost of Transgression (UCT)** theorem, which quantitatively links the thermodynamic cost of acceleration (via the Unruh effect) to the resource cost of high-fidelity prediction, providing a physical mechanism for the principle of Prediction Relativity (derived in Appendix N).
*   Specific, falsifiable experimental proposals designed to test the novel predictions of the framework, particularly the CC hypothesis.

**1.4 Paper Organization**

This paper is structured as follows:

*   **Section 2:** Establishes foundational principles: Axioms (POP, Predictive Capacity), definitions (Information, Prediction-Based Knowledge), Predictive Physical Complexity ($C_P$), operational proxy ($\hat{C}_v$) and dynamical alignment justification, resource cost functions ($R, R_I$) and operators ($\hat{R}, \hat{R}_I$), and necessary conditions for prediction.
*   **Section 3:** Details the dynamics of prediction: the Fundamental Predictive Loop, Reflexive Interaction Dynamics (RID), and derives the necessity of the Space of Becoming $(\alpha,\beta)$.
*   **Section 4:** Explores self-reference, proves the SPAP theorems, introduces Reflexive Undecidability, Logical Indeterminacy, and analyzes complexity dynamics near prediction limits.
*   **Section 5:** Defines the Operational Threshold $C_{op}$ for the adaptive loop, identifies the distinct Horizon Constant $K_0$ as the minimum complexity for SPAP logic, and establishes the relationship $C_{op} \ge K_0$.
*   **Section 6:** Introduces the Principle of Compression Efficiency (PCE), derives the Law of Prediction within $(\alpha,\beta)$, and develops the complexity adaptation dynamics driven by the force $\Psi(t)$.
*   **Section 7:** Presents the Minimal Predictive Unit (MPU) framework (Hypothesis 1): definition, Perspectival State, Dual Dynamics, and derives crucial thermodynamic constraints ($\varepsilon \ge \ln 2$, $\kappa_r > 0$).
*   **Section 8:** Demonstrates the emergence of the quantum mechanical formalism (Hilbert space, Born rule, uncertainty, entanglement, Schrödinger equation) from MPU dynamics.
*   **Section 9:** Introduces the Consciousness Complexity (CC) hypothesis (Hypothesis 3): emergence, operational definition, scaling, mechanism, and modeling modified probabilities.
*   **Section 10:** Details the framework's stance on causality (Postulate 2), derives the CC bound ($\alpha_{CC,max} < 0.5$), introduces the statistical FTL hypothesis (Postulate 3), and analyzes consistency.
*   **Section 11:** Details the conditional emergence of spacetime geometry from the MPU network, relying on necessary geometric regularity (Theorem 43).
*   **Section 12:** Presents the thermodynamic derivation of Einstein's Field Equations (Theorem 50) from MPU network properties and the Area Law (Theorem 49).
*   **Section 13:** Outlines specific experimental protocols to test the CC hypothesis and other predictions.
*   **Section 14:** Provides discussion, including philosophical implications, comparisons to other frameworks, limitations, and challenges.
*   **Section 15:** Offers concluding remarks.
*   **Glossary:** See Glossary of Key Terms for complete definitions.
*   **References:** Provided for all cited works.
*   **Appendices (A-Y):** Provide detailed proofs, derivations, and analyses supporting the main text's arguments.

**1.5 Logical Structure of the Framework**

The Predictive Universe (PU) framework is a deductive theoretical structure built upon operationalizing the philosophical certainty of the Cogito. It derives physical law not as an external set of rules, but as the emergent, resource-efficient embodiment of logical and predictive necessities. This bridge between logic and physics is formalized by the **Principle of Physical Instantiation (PPI)** (detailed in Appendix P), which posits that any abstract logical requirement, when implemented by a physical system with finite resources, is shaped by irreducible thermodynamic costs and optimization imperatives like the Principle of Compression Efficiency (PCE). The framework's core arguments unfold through three pillars, each an application of this core methodology.

**Pillar I: Emergence of Quantum Mechanics from the Logic of Self-Reference**

This pillar demonstrates how the formalism of quantum mechanics arises as the necessary, resource-efficient language for self-referential predictive systems.

1.  **Foundations in Prediction & Logic:** Starting from the predictive nature of consciousness, the framework establishes that any predictive system must operate via a **Fundamental Predictive Loop** (Definition 4) and possess sufficient computational richness (**Property R**, Definition 10). The framework argues that this computational capability is not an arbitrary assumption but is itself a necessary emergent feature, dynamically enforced by the system's drive to optimize its predictive efficiency (**Appendix A.0**).
2.  **Logical Indeterminacy & The Quantum of Complexity:** The application of Property R to self-prediction leads to the **Self-Referential Paradox of Accurate Prediction (SPAP)** (Theorems 10, 11), proving a fundamental **Logical Indeterminacy** (Definition 12). This provides a candidate origin for quantum randomness (Hypothesis 2). The logic of SPAP requires a minimum of 3 bits of complexity, establishing the **Horizon Constant $K_0=3$** (Theorem 15) and constraining the minimal MPU Hilbert space to $d_0 \ge 8$ (Theorem 23).
3.  **Quantum Formalism from Optimization:** This is a crucial step. The framework demonstrates that Logical Indeterminacy necessitates a probabilistic description of outcomes. The Principle of Compression Efficiency (PCE, Definition 15), when applied to the problem of optimally and consistently assigning these probabilities via a non-contextual "cost frame function," is shown to uniquely select the **Born rule** for probabilities (**Appendix G.1**, Theorem G.1.7). Crucially, the same optimization principles also demonstrate that the **complex Hilbert space** is the unique, stable algebraic structure for representing predictive states, as alternative formalisms are shown to be less resource-efficient (**Appendix G.1**, Theorem G.1.8). The core quantum formalism is thus not postulated but emerges as the most efficient solution.
4.  **Dual Dynamics & Prediction Relativity:** With the Hilbert space and Born rule established, the MPU's operational cycle maps directly onto the **Dual Dynamics** of quantum theory: unitary evolution (Internal Prediction) and stochastic interaction ('Evolve'). The SPAP limit on prediction, $\alpha_{SPAP}$, is framed as a **Prediction Coherence Boundary**, analogous to the speed of light. The framework formalizes this as **Prediction Relativity** (Remark 3), unifying the divergent costs of approaching both limits via the **Unified Cost of Transgression (UCT)** theorem (derived in **Appendix N**), which links them through a shared thermodynamic cost.
5.  **Standard Model Gauge Group, Generations, and Hierarchy:** The PCE-driven emergence of gauge theory is shown to extend beyond simple U(1) electromagnetism. A comprehensive argument in **Appendix G.8** posits that the **Standard Model (SM) gauge group $SU(3)\times SU(2)\times U(1)$** and the **D=4 dimensionality of spacetime** are co-selected as a unified, PCE-optimal structure. This selection arises from minimizing a global PCE potential subject to several D-sensitive constraints: (i) the MPU network's intrinsic information capacity limits the total number of gauge generators to $n_G \lesssim 20$; (ii) mathematical consistency requires the theory to be anomaly-free; and (iii) D=4 is uniquely favored for its ability to support the stable, complex MPU aggregates necessary for advanced prediction. The SM, with $n_G=12$, fits the capacity constraint and is famously anomaly-free in D=4, emerging as a uniquely efficient solution. A further topological argument, detailed in **Appendix R**, demonstrates that the **three-generation structure of SM fermions** arises from the topology of the MPU's internal Perspective Space ($\pi_2(U(8)/T^8) \cong \mathbb{Z}^7$). PCE-driven selection for non-Abelian charge neutrality and Abelian anomaly cancellation across distinct topological sectors is shown to uniquely favor a minimal three-sector solution, thereby explaining the family problem from first principles. Finally, the framework provides a substrate-first derivation of the **electroweak/Planck hierarchy**. As detailed in **Appendix T**, the electroweak VEV emerges from a PCE-driven competition between a stabilizing, area-law boundary cost and a destabilizing, logarithmically running bulk mass term in a coarse-grained effective potential. This mechanism generates an exponentially large critical scale $\ell^*$ that sets the VEV, explaining the hierarchy without fine-tuning and yielding the falsifiable prediction that the electroweak scale increases with the MPU's information capacity (`∂ln(v)/∂ln(C_max) > 0`). The full set of SM Yukawa couplings and the Higgs self-coupling are posited to emerge from the same substrate dynamics, representing a key program for future work. The framework also provides a natural origin for the **baryon asymmetry of the universe**, showing that the Sakharov conditions are generic consequences of the emergent gauge structure and thermodynamics, with the net baryon number arising from anomaly inflow (**Appendix Y**). This pillar culminates in a rigorous pathway (**Appendix G.9**) to derive the value of the fine-structure constant, $\alpha_{em}$. The calculation is framed as a rate-level PCE optimization, yielding a unique, non-zero equilibrium coupling from the MPU's fundamental parameters ($d_0=8$, $\varepsilon=\ln 2$). This, along with an inversion linking the observed cosmological constant $\Lambda$ to the instanton complexity $\kappa$, is consolidated into a concrete computational program in Appendix V. Appendix W further develops the framework by deriving robust, testable bounds—and, at a symmetric reference point, a parameter-free identity—linking fundamental constants to information-theoretic invariants of the MPU cycle, providing a stringent internal-consistency check.

**Pillar II: Emergence of Spacetime and Gravity from the Thermodynamics of Interaction**

This pillar derives general relativity as a macroscopic thermodynamic consequence of the MPU network's information-limited interactions.

1.  **The Thermodynamic Ratchet and the Arrow of Time:** The physical instantiation of the SPAP logic within the 'Evolve' process is shown to carry an irreducible, irreversible thermodynamic cost **$\varepsilon \ge \ln 2$** (Theorem 31, derived in **Appendix J** from the necessary logical state-merging of the finite-memory SPAP cycle). This ubiquitous microscopic cost acts as a thermodynamic ratchet that physically enforces the unidirectional **arrow of time** (derived in **Appendix O**), which is itself a logical necessity for prediction (Theorem 4).
2.  **Information Channel Limits:** The fundamental cost $\varepsilon$ ensures MPU interaction channels are information-limited ($C_{max} < \ln d_0$) (**Appendix E**). This establishes a universal bottleneck on information flow.
3.  **The Geometric Stage:** PCE optimization is shown to drive the MPU network towards **Geometric Regularity** (Theorem 43), creating a stable, regular network structure that can be described as a continuous manifold (justified in **Appendices C & D**).
4.  **Area Law and Gravity's Scale:** On this regular geometric stage, the information channel limits (from step 2), which arise from the channel's strict contractivity due to the $\varepsilon$-cost, lead directly to the **Horizon Entropy Area Law** (Theorem 49, derived in **Appendix E**). This derivation also defines the emergent **Gravitational Constant $G$** in terms of underlying MPU network parameters.
5.  **Einstein's Field Equations:** Applying local thermodynamic principles (Postulate 4) to causal horizons, using the derived Area Law and the **MPU Stress-Energy Tensor** ($T_{\mu\nu}^{(MPU)}$ from **Appendix B**) as the source of heat flow, uniquely yields **Einstein's Field Equations** (Theorem 50) as the system's equation of state.

**Pillar III: Emergence of Consciousness Complexity (CC) and a Unified Dark Sector**

This pillar proposes novel, testable hypotheses for how complex predictive processing interfaces with fundamental physics and cosmology.

1.  **Emergent Biasing Capability (CC):** Given that fundamental ND-RID parameters can depend on local context (Assumption 1, motivated by PCE favoring adaptive interaction channels and constrained by **Appendix L**), PCE optimization in complex MPU aggregates ($C_{agg} > C_{op}$) is shown to necessarily lead to an **Emergent Biasing Capability** (Theorem 34). This capability, which arises from the aggregate learning to modulate its internal state to favorably influence local interaction outcomes, is operationally quantified as **Consciousness Complexity (CC)** (Definition 30).
2.  **Causality and Statistical FTL:** To preserve operational causality (Postulate 2), CC is rigorously bounded: **$\text{CC} < 0.5$** (Theorem 39). This bounded mechanism, acting on entangled systems, opens the possibility of **Statistical FTL Influence** (Postulate 3), which is argued to be consistent with causality due to fundamental information limits (Theorems 40-42, supported by the formal AQFT analysis in **Appendix F**). This leads to specific, falsifiable experimental predictions (Section 13).
3.  **Scale-Dependent Gravity and the Dark Sector:** The same overarching **Principle of Compression Efficiency (PCE)** is extended to cosmology. It is argued that PCE, acting through a different mechanism—namely, the adaptation of fundamental MPU network parameters to the large-scale information environment—causes the emergent gravitational constant $G$ to become scale-dependent. This provides a potential unified explanation for the "dark matter" phenomenology observed in galaxies (**Appendix I**) and aspects of the "dark energy" puzzle related to cosmic expansion (**Appendix K**), framing both as manifestations of gravity's adaptation to the information environment.

These three pillars, built upon the same philosophical and physical foundations, demonstrate the framework's potential to provide a coherent and deductive account of quantum mechanics, gravity, and consciousness-related phenomena as emergent features of a unified, predictive reality.