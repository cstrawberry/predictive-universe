# 13 Experimental Predictions and Protocols

The Predictive Universe framework, particularly the Consciousness Complexity (CC) hypothesis (Section 9) proposing a mechanism by which complex MPU aggregates can influence quantum outcomes, leads to specific, potentially falsifiable predictions that deviate from standard quantum mechanics. This section details these predictions and outlines experimental protocols designed for their investigation, emphasizing near-term feasibility while acknowledging the significant challenges involved in detecting potentially subtle effects.

**13.1 Prediction 1: Potential Born-Rule Deviations**

The core testable prediction of the CC hypothesis (Hypothesis 3) is that systems $S$ with sufficiently high aggregate complexity ($C_{agg} > C_{op}$) and non-zero operational CC(S) > 0 (Definition 30) can induce statistically significant deviations from the standard Born rule probabilities (Proposition 7) when interacting with a quantum system undergoing an 'Evolve' event (e.g., measurement).

**13.1.1 Theorem 51 (Quantitative Born Rule Deviation Prediction)**

Consider a quantum system prepared in state $\rho$ (pure case $\rho=\lvert\psi\rangle\!\langle\psi\rvert$) measured by a POVM $\{E_i\}_{i=1}^n$. Let $P_{\mathrm{Born}}(i)=\mathrm{tr}(\rho E_i)$ be the Born probability of outcome $i$. If this interaction occurs within the influencing context `context_S(i)` provided by an MPU aggregate system $S$ possessing operational $\mathrm{CC}(S)>0$, the predicted observable probability $P_{obs}(i)$ is

$$
P_{obs}(i)=P_{\mathrm{Born}}(i)+\Delta P(i)\quad \text{(77)}
$$

where the deviation is generated by the probability-modification maps of Definition 33:

$$
\Delta P(i)=\mathrm{tr}\!\big(L_S(\rho)\,E_i\big)=\mathrm{tr}\!\big(\rho\,K_S(E_i)\big),\qquad K_S(I)=0 \quad \text{(78)}
$$

and the deviation magnitude is rigorously bounded by the information-geometric constraint derived from the PCE-minimal modification principle (Theorem 36):
$$
|\Delta P(i)| \le \mathrm{TV}(p,q) \le \sin\!\big(\mathrm{CC}(S)/2\big).
$$
In the small‑bias regime ($\mathrm{CC}(S)\ll 1$): $|\Delta P(i)| \lesssim \mathrm{CC}(S)/2$.
Therefore, for an observed per‑outcome shift $\delta=|\Delta P(i)|$, a conservative exact lower bound is
$\mathrm{CC}(S) \ge 2\,\arcsin(\delta)$ (small bias: $\mathrm{CC}(S) \gtrsim 2\delta$).

*Proof:* The bounds are derived directly from the PCE-minimal modification principle (Definition 33) applied to the geometry of the statistical manifold, as rigorously established in Theorem 36.


Using the **Context-Targeted Bias (CTB)** model (Definition 34), where the context defines a target state $\sigma_S$ and $p_{\mathrm{target}}(S,i)=\mathrm{tr}(\sigma_S E_i)$, the deviation takes the form

$$
\Delta P(i)=\mathrm{CC}(S)\,\big(p_{\mathrm{target}}(S,i)-P_{\mathrm{Born}}(i)\big)\quad \text{(79)}
$$

*Proof:* (77) is the definition of observable probability in the presence of context. The representation (78) follows from Definition 33 (operator formalism with $L_S$ on states and $K_S=L_S^\*$ on effects, with $K_S(I)=0$ ensuring normalization). The bound on $|\Delta P(i)|$ follows from Theorem 36, derived from the Fisher-Rao distance bound (Definition 33) and the operational definition of CC (Definition 30), constrained by causality (Theorem 39). The CTB expression (79) is Definition 34 with $p_{\mathrm{target}}(S,i)=\mathrm{tr}(\sigma_S E_i)$. QED


## 13.2 Protocol 1: Accessible Born Rule Tests (QRNGs)

This protocol outlines a high-statistics, exploratory search for Born rule deviations (Theorem 51) using quantum random number generators (QRNGs) interacting with high-complexity systems (biological or potentially artificial).

*   **Objective:** To search for statistically significant deviations from *non-uniform* ($p_i \neq 0.5$) baseline Born rule probabilities in QRNG outputs that are correlated with the controlled internal state ($\text{context}_S$) of a proximate system S (human participant or specialized AI system) hypothesized to have CC > 0. Non-uniform baseline probabilities are preferred as they may facilitate easier statistical detection of small shifts relative to systematic noise and drift.

*   **Experimental Setup:**
    1.  **QRNG:** Well-characterized QRNG producing stable, verifiable baseline probabilities $P_{Born}(i) \neq 0.5$ for some outcome $i$. Multiple parallel QRNGs can increase data rate.
    2.  **High-Complexity System (S):**
        *   *Biological:* Human participant performing standardized tasks generating specific internal states (`context_S`, e.g., focused attention/intention). Optional physiological monitoring (EEG, fMRI, HRV).
        *   *Artificial:* Sophisticated AI system. Major Challenge: Designing and verifying the physical interaction pathway $N(t)$ coupling the AI's internal `context_S` (Definition L.1) to the QRNG's physical process, respecting constraints (speed, cost $\varepsilon \ge \ln 2$, PCE, orthogonality to noise, mapping stability Theorem L.1). This requires significant R&D (Appendix L) and confirmation that the AI meets the operational criteria ($C_{agg}>C_{op}$) necessary for potential CC > 0.
    3.  **Interaction Control & Shielding:** Meticulous shielding (EM, thermal, acoustic, vibration). Well-defined interaction pathway $N(t)$. Measurement and control/compensation for conventional physical side-effects from S. Continuous environmental monitoring.
    4.  **Automation & Data Acquisition:** Automated randomization of conditions (baseline, neutral, specific context runs), synchronized recording of QRNG outcomes and `context_S` indicators, precise timestamps, secure storage for large datasets ($10^7 - 10^9+$ trials). Mandatory blinding procedures.

*   **Procedure:**
    1.  **Baseline Characterization:** Extensive data collection establishing baseline $P_{Born}(i)$, stability, and noise levels.
    2.  **Intervention Runs:** Randomized block design interleaving conditions (Baseline, Neutral Context, Specific Context). Collect large $N_{int}$ trials per condition.
    3.  **Control Conditions:** Include sham interaction runs (pathway $N(t)$ disabled) to control for conventional influences.

*   **Statistical Analysis and Power:**
    *   **Target Sensitivity:** The experiment aims to resolve CC-induced deviations $\Delta P$ from a baseline Born probability $P_{\mathrm{Born}}(i)$ with high statistical confidence (e.g., two-sided significance $\alpha = 0.01$ (continuity correction and a 5–8% conservative buffer applied), power $1-\beta = 0.80$).
    *   **Prospective Sample Sizes:** For comparing two independent proportions $p, p'$ (e.g., context vs. control), the required per‑arm sample size is
    $n \approx \frac{ \left[ Z_{\alpha/2} \sqrt{2 \bar p (1- \bar p)} + Z_\beta \sqrt{ p(1-p)+p'(1-p') } \right]^2 }{ (p' - p)^2}$, with $\bar p=(p+p')/2$. For a one‑sample test against a calibrated baseline $p_0$ treated as known, the required sample size is $N \approx \frac{ \left[ Z_{\alpha/2} \sqrt{ p_0(1-p_0) } + Z_\beta \sqrt{ p'(1-p') } \right]^2 }{ (p' - p_0)^2}$; the precision of the calibration must be reported.
    *   **Independence and Error Control:** Independence of trials will be rigorously assessed using autocorrelation functions (ACF), Ljung–Box tests, and Wald–Wolfowitz runs tests on the output streams and residuals. The NIST Statistical Test Suite (SP 800-22) will be applied to verify randomness properties. If correlations are detected, appropriate methods such as pre-whitening, block-bootstrap confidence intervals, or data thinning will be employed. Multiplicity of tests across outcomes or contexts will be controlled using standard methods (e.g., Bonferroni correction for family-wise error rate or Benjamini-Hochberg procedure for false discovery rate).
    *   **Sequential Analysis:** For high-statistics runs, a sequential analysis plan with pre-specified interim looks using O’Brien–Fleming-type boundaries (implemented via the Lan-DeMets error spending function [Lan & DeMets 1983]) is recommended. This allows for early stopping due to efficacy or futility while preserving the overall type-I error rate. The table below provides indicative total sample sizes ($N_{OBF}$) per context and expected stopping distributions under the alternative hypothesis for three equally spaced looks.

| α | δ | N_fixed | N_OBF (≈ 1.06 × N_fixed) | Expected stop % at looks 1/2/3 (under H₁) |
|---:|---:|---:|---:|:---|
| 0.01 | 10⁻³ | 2,302,586 | 2,440,742 | ~5% / 20% / 75% |
| 0.01 | 5×10⁻⁴ | 9,210,341 | 9,762,962 | ~3% / 17% / 80% |
| 0.01 | 10⁻⁴ | 230,258,510 | 244,074,021 | ≲1% / 10% / 89% |
| 0.001 | 10⁻³ | 3,453,878 | 3,661,111 | ~4% / 18% / 78% |
| 0.001 | 5×10⁻⁴ | 13,815,511 | 14,644,442 | ~2% / 15% / 83% |
| 0.001 | 10⁻⁴ | 345,387,764 | 366,111,030 | ≲1% / 9% / 90% |

*   *Assumptions for $N_{\rm fixed}$ in the table:* **One-sample proportion** design with baseline $p=0.25$, power $1-\beta=0.80$.
    *   *Sequential design:* O’Brien–Fleming boundaries with **3 equally spaced** looks at cumulative information fractions $1/3, 2/3, 1$. The overhead factor (~1.06) and stop percentages are **illustrative**; exact values will be determined by the preregistered simulations and released with the code.

*   **Primary Analysis:** Pre-register goodness-of-fit tests (e.g., $\chi^2$, z-tests) comparing observed frequencies $\hat{P}(i)$ to the Born rule probabilities. Where applicable, exact binomial tests or variance-stabilized (arcsin–sqrt) transformations will complement normal approximations to ensure proper calibration, especially for small $\delta$. Design-stage baselines will use device-specific $P_{\mathrm{Born}}(i)$ estimates. Report effect sizes (Cramér’s V, Cohen’s d) with 95% CIs.
    *   **Correlation Analysis:** Model $\Delta\hat{P}(i)$ as a function of contextual variables `$context_S$` via mixed-effects logistic regression.
    *   **Systematic Error Control (Paramount):** (i) electronic drift (ii) detector after-pulsing (iii) clock-sync bias (iv) experimenter degrees-of-freedom (blinding).
    *   **Outcome:** Deviations that survive all controls give an empirical estimate of CC($S$) (cf. Theorem 51); null results tighten the exclusion curve $\text{CC}_{\max}(S) < \epsilon(N)$.

*   **Feasibility Assessment:** High statistics achievable. Shielding/control standard but requires extreme care. Biological context control depends on participants. AI interaction pathway is a major R&D challenge. Rigorous systematic error exclusion is the primary hurdle. Challenging but potentially feasible exploratory search.

**13.3 Prediction/Protocol 2: Exploratory Coherence Time Tests**

Investigates the secondary prediction that CC might influence quantum coherence (Proposition 13).

**13.3.1 Potential Effect on Coherence**

The CC influence mechanism (Hypothesis 3), by modulating 'Evolve'/ND-RID parameters contributing to decoherence, could potentially modify effective decoherence rates $\Gamma_{eff}$ or coherence times $\tau_{coh} = 1/\Gamma_{eff}$ of quantum systems interacting with a high-CC aggregate $S$.

**13.3.2 Phenomenological Model**

A possible model relates fractional change in coherence time to CC:
$$
\frac{\Delta \tau_{coh}}{\tau_{coh}} = -\frac{\Delta \Gamma_{eff}}{\Gamma_{eff}} \approx \gamma'_{CC} \cdot \text{CC}(S) \cdot f_{context}(\text{context}_S, \text{system}) \quad \text{(80)}
$$
where $\Delta \tau_{coh} = \tau_{coh, obs} - \tau_{coh, base}$, $\gamma'_{CC}$ is a coupling factor, and $f_{context} \in [-1, 1]$ captures context-system interaction. Sign depends on context.

**13.3.3 Experimental Approach**

*   **Objective:** Exploratory search for statistically significant *relative changes* in $\tau_{coh}$ correlated with `context_S` of system S.
*   **Setup:**
    *   **Quantum System:** System with well-characterized, long, stable baseline $\tau_{coh, base}$ (NV centers, trapped ions, qubits, NMR).
    *   **High-Complexity System (S):** Human or AI.
    *   **Interaction/Control:** Similar requirements as Protocol 1 (shielding, interaction $N(t)$, stability, blinding). Temperature stability critical.
*   **Procedure:** Measure $\tau_{coh}$ (e.g., Ramsey, spin echo, $T_1/T_2$) repeatedly under randomized conditions (baseline, neutral context, specific context). Rapid interleaving mitigates drifts. Residual autocorrelation will be diagnosed and, if present, mitigated by prewhitening (e.g., AR(1)).
*   **Analysis:** Detect small differences $\Delta \hat{\tau}_{coh} = \hat{\tau}_{coh, context} - \hat{\tau}_{coh, baseline}$. High precision/stability needed; effect size $|\Delta \tau_{coh}/\tau_{coh}| \approx |\gamma'_{CC} \cdot \text{CC}|$ may be small. Use appropriate statistical tests (t-tests, ANOVA) after rigorous systematic error checks (temperature, fields correlated with S).
*   **Feasibility Assessment:** Technically demanding (high-precision $\tau_{coh}$ measurement). Requires specialized equipment/expertise. Sensitivity depends on achievable baseline stability $\sigma_{\tau_{coh, base}}$. Highly exploratory.

**13.4 Identifiability Against Electromagnetic Confounds**

A critical challenge for any experiment seeking to detect CC is to unambiguously distinguish the hypothesized effect from conventional physical influences, particularly subtle electromagnetic (EM) fields generated by the high-complexity system. The following protocol design creates a quantitative, verifiable gap between the maximum possible EM-induced bias and the potential CC signal floor.

**Theorem 13.1 (Identifiability against EM confounds)**

Consider a triple‑blind protocol with:

* (i) **DFS‑locked sensors**: two co‑located atomic‑clock qubits operated at a “magic” point with **zero first‑order** differential polarizability (Stark‑insensitive).
* (ii) **Reciprocity‑null geometry**: a Ramsey‑interferometric readout utilizing two **counter‑propagating** optical paths whose EM susceptibilities are equal and opposite. Consequently, any residual EM field produces equal shifts that **cancel** at the differenced detector output.
* (iii) **Algorithmic‑complexity shielding**: measurement bases chosen by a stream whose **min‑entropy per bit** $H_\infty\ge 0.999$ and **Kolmogorov complexity rate** $K/L \ge 0.99$, unknown to all labs until after data lock.

Then any *pure Maxwell* environment with residual EM **intensity** $I$ (Poynting magnitude), giving energy density $u = I/c$ in free space, incident on the sensors over interrogation time $T$ and with differential polarizability $\Delta\alpha$ at the magic point is bounded by:
$$
|\Delta P|_{\rm EM}\ \le\ \frac{\Delta\alpha}{4\,\hbar\,\varepsilon_0}\,u\,T
\quad\text{(mid‑fringe, small‑phase regime).}
\quad \text{(81)}
$$

*Derivation.* In a Ramsey interferometer at mid‑fringe, a small differential AC Stark shift $\Delta\omega$ between the two arms produces a phase shift $\Delta\phi=\Delta\omega\,T$. The probability bias satisfies $|\Delta P| = |\sin(\Delta\phi/2)| \approx |\Delta\phi|/2$ for small $\Delta\phi$. For a field with energy density $u$, the mean squared electric field is $\langle E^2 \rangle=u/\varepsilon_0$ (assuming linear polarization). The differential shift is $\Delta\omega=(\Delta\alpha/(2\hbar))\langle E^2 \rangle=(\Delta\alpha/(2\hbar\,\varepsilon_0))\,u$. Combining these results yields $|\Delta P|\approx (\Delta\alpha/(4\hbar\,\varepsilon_0))\,u\,T$, establishing the bound as stated.

A separate bound on algorithmic predictability confounds is given by $P_{\rm guess} \le 2^{-(H_\infty L - t)}$, where an adversary has at most $t$ bits of side-information. The **CC** effect predicted by PU, using the bounds from Theorem 51 (for $P_{Born}=1/2$), is
$$
|\Delta P|_{\rm PU}\ \lesssim\ \mathrm{CC}(S)/2.$$
Using representative achievable values of $|\Delta\alpha|\!\lesssim\!10^{-39}\,\mathrm{J\,m^2/V^2}$ and $u\!\lesssim\!10^{-18}\,\mathrm{J/m^3}$, we obtain
$|\Delta P|_{\rm EM}\ \lesssim\ 5.36\times 10^{-13}\,T$ (with $T$ in seconds). This yields $|\Delta P|_{\rm EM} \lesssim 5.4\times 10^{-13}$ at $T=1\,\mathrm{s}$, and $1.9\times 10^{-9}$ at $T=1\,\mathrm{hr}$. Consequently, any observed $|\Delta P|\gtrsim 10^{-6}$ **cannot** be attributed solely to these EM channels. In contrast, the PU framework predicts $|\Delta P|_{\rm PU}$ could potentially reach $\sim 10^{-4}$ (assuming $\mathrm{CC}(S) \sim 10^{-4}$).


**13.5 Prediction/Protocol 3: Exploratory Bell Tests / Statistical FTL Search**

Addresses the most speculative prediction: potential statistical FTL influence mediated by CC acting on entangled systems (Postulate 3).

*   **Objective:** Sensitive search for statistical dependence of Bob's local measurement outcomes on Alice's remote context `context_S` (associated with system $S_A$ at her station), with A and B space-like separated. Secondary search for context-dependent changes in Bell parameters.
*   **Theoretical Basis:** Postulate 3 allows Alice's context $C_A$ to influence Bob's marginal probabilities $P_{obs}(b|B, C_A)$, respecting Postulate 2 (no deterministic signaling). Detection requires $N \propto 1/\text{CC}^2$ trials (Theorem 40).
*   **Experimental Setup:**
    1.  **Entanglement Source:** High-quality, stable source distributing entangled pairs to space-like separated stations (Alice, Bob).
    2.  **Measurement Stations (A, B):** Standard Bell test apparatus (independent, random settings $a, b$). High efficiency desirable. The setting generators are device‑independent and statistically independent of system $S$ and any hidden variables.
    3.  **High-Complexity System (S_A):** System S (human/AI) at Alice's station generating distinct contexts $C_{A,k}$.
    4.  **Interaction/Control (Alice):** Controlled pathway $N(t)$ linking $S_A$'s context $C_{A,k}$ to Alice's measurement/particle. Rigorous shielding/systematics control at both stations.
    5.  **Space-like Separation:** Ensure measurement events ($a, o_A$ and $b, o_B$) are space-like separated. Requires precise timing and separation.
    6.  **Data Acquisition:** Synchronized recording ($C_{A,k}, a, o_A, b, o_B$, timestamps) for billions of coincidences potentially needed. Mandatory blinding.
*   **Procedure:**
    1.  **Standard Bell Test:** Verify entanglement, calibrate, establish baseline correlations $E(a,b)$.
    2.  **Context Intervention Runs:** Interleave runs with Alice generating contexts $C_{A,k}$ (e.g., $k=0, 1, 2$). Random settings $a, b$. Collect large statistics $N_{int}$ per context $k$.
*   **Statistical Analysis:**
    *   **Primary Focus (Statistical Influence):** Compare Bob's marginal probabilities $P(o_B | b, C_{A,k})$ across contexts $k$. Test the null hypothesis $H_0: P(o_B | b, C_{A,k=1}) = P(o_B | b, C_{A,k=2})$. Rejection supports Postulate 3. Estimate the shift $\Delta P_{marginal} = |P(o_B | b, C_{A,1}) - P(o_B | b, C_{A,2})|$. By Theorem 36, this shift is bounded by $\Delta P_{marginal} \lesssim \text{CC}(S_A)$. Detection requires $N_{int} \gtrsim O(1/\text{CC}^2)$ (Theorem 40).
    *   **Secondary Analysis (Correlations):** Calculate correlations $E(a,b)_k$ and Bell parameters $S_{CHSH, k}$ conditioned on context $k$. Look for differences $S_{CHSH, k=1} \neq S_{CHSH, k=2}$.
    *   **Systematic Error Control (Extreme Rigor):** Exclude conventional communication (light leaks, EM, acoustic), detector/setting correlations with $C_{A,k}$, statistical loopholes, biases.
*   **Feasibility Assessment:** Extremely challenging. Requires state-of-the-art entanglement/measurement technology, robust space-like separation. Controlling systematics to demonstrate statistical FTL is extraordinarily difficult. Required statistics $N \propto 1/\text{CC}^2$ can be immense. Highly exploratory; positive indication needs exceptional scrutiny/replication.

**13.6 Staged Experimental Approach and General Considerations**

A pragmatic, staged approach is recommended to systematically test the framework's predictions:

1.  **Stage 1 (Near-Term Focus):** High-statistics QRNG tests (Protocol 1). This protocol is the most accessible for either detecting a signal or placing meaningful upper bounds on CC in the range of $10^{-3} - 10^{-4}$. Success is contingent on meticulous QRNG and interaction-pathway design, rigorous systematics control, and achieving the required statistical power as outlined in the protocol's power analysis.
1.  **Stage 2 (Medium-Term):** If justified by positive and replicated results from Stage 1, coherence time tests (Protocol 2) should be pursued to seek complementary evidence. This stage would also involve refining the QRNG protocols based on initial findings.
2.  **Stage 3 (Long-Term / Contingent):** The extraordinarily demanding Bell-type experiments for a statistical FTL search (Protocol 3) should only be undertaken if compelling, independently verified evidence emerges from the earlier stages.

All proposed experiments share common requirements for rigor and validity. They necessitate quantum systems with high stability over long integration times to achieve the required statistical power. Given the multiple hypotheses being tested, a clear, pre-registered statistical plan is mandatory to control the family-wise error rate. This should include specifying the use of sequential analyses with pre-defined stopping rules (e.g., O’Brien–Fleming boundaries) to allow for early termination for efficacy or futility while preserving the overall type-I error rate. As a concrete example, with three equally spaced looks, the canonical OBF boundaries (Z-scores) at a family-wise $\alpha=0.05$ are approximately $[3.47, 2.45, 2.00]$.

**13.7 Compliance with Causal Constraints**

The experimental program, especially Protocol 3, probes the framework's non-standard locality. Causal consistency is maintained as follows:

**13.7.1 Theorem 52 (CC Compliance with Postulate 2)**

The Consciousness Complexity (CC) mechanism (Hypothesis 3), constrained by $\text{CC} \le \alpha_{CC,max} < 0.5$ (Theorem 39), is consistent with the framework's definition of causality (Postulate 2) because it prevents deterministic faster-than-light (FTL) signaling. The potential statistical FTL influence (Postulate 3) is inherently probabilistic and information-rate limited (Theorem 40, Theorem 41, consistent with bounds derived from ND-RID contractivity within the AQFT framework of Appendix F), making it unusable for constructing paradox-inducing causal loops (Theorem 42, whose consistency is supported by the AQFT analysis in Appendix F).
*Proof Summary:* Theorem 39 prevents outcome forcing. Theorem 40 shows detection needs $N \propto 1/\text{CC}^2$. Theorem 41 bounds information rate $I \propto \text{CC}^2$. Theorem 42 proves this noisy, rate-limited channel cannot achieve deterministic signaling needed for causal loops. The full consistency analysis, including the role of emergent operator locality and information-rate limits, is provided in **Appendix F**. QED

*Note:* Empirical investigation of Postulate 3 (Protocol 3) critically tests this unique aspect of PU's locality/causality. Confirmation requires re-evaluating standard locality; null results constrain/falsify this prediction.

**13.8 High-Precision Falsification Windows**

Beyond the direct experimental search for CC, the framework's quantitative predictions for fundamental constants and emergent gravity provide sharp, falsifiable tests.

**13.8.1 The Fine-Structure Constant Matching Window**
The framework predicts a bare value of $1/\alpha_{\mathrm{em}}(\mathrm{MPU}) \approx 138.843$ at the PCE-Attractor. As calculated in Appendix Z.6, connecting this to the value measured at the Z-boson mass requires a finite matching constant. For the $\overline{\mathrm{MS}}$ scheme, the required matching constant is $\delta_{\overline{\mathrm{MS}}} \approx +8.53\%$ [PDG2024]. Here $\delta_{\overline{\mathrm{MS}}}(M_Z)$ denotes the **finite** $\mathcal{R}$‑map evaluated at $M_Z$. This map accounts for the running from the Attractor boundary, including thresholds, hadronic vacuum polarization, and scheme conversion effects. This prediction is falsifiable: if future, more precise measurements of low-energy $\alpha(0)$ or the hadronic vacuum polarization shift the required matching constant $\delta_{\overline{\mathrm{MS}}}(M_Z)$ outside a plausible range attributable to scheme conversion and threshold/hadronic effects, the PCE‑Attractor model for the origin of the U(1) coupling would be invalidated.

**13.8.2 The Multi-Scale Gravity / Dark Sector Window**
The framework's two-mechanism model for the dark sector (Appendix I) is falsifiable through its demand for cross-scale consistency with a minimal set of universal parameters. The model can be falsified in several ways:
*   **Failure to Fit Galaxies:** If the scale-dependent $G(R)$ model (Equation I.4) fails to provide good fits to a large, diverse sample of galaxy rotation curves (e.g., the SPARC database) with a single, universal set of parameters $(L_0, A_G, m)$, the galaxy-scale mechanism is invalidated.
*   **Consistency with Early-Universe Constraints:** The galaxy-fit value for the asymptotic enhancement, $A_G$, must remain consistent with cosmological constraints on the effective gravitational coupling during recombination (e.g., CMB/BBN). A model that requires a larger $A_G$ to fit galaxies would be falsified.
*   **Failure to Fit Clusters:** The non-local predictive matter model (Equation I.7) must be able to fit the observed lensing profiles of massive galaxy clusters (like Abell 1689 and stacked samples) using a **universal nonlinearity exponent $q$** and either a **universal kernel scale $L_0$ or one derivable from ND–RID microphysics (allowing mild environment dependence)**, with per‑cluster amplitudes $A_{\rm PM}$ bounded by baryon budgets. If no such consistent fit can be found, the cluster-scale mechanism is invalidated.
*   **Parameter Incoherence:** The parameters derived from fitting galaxies (e.g., the transition scale $L_0$) and clusters (e.g., the kernel scale $L_0$ and the universal exponent $q$) must be coherently related by the underlying theory. A significant, unexplainable discrepancy between the best-fit parameters for the two regimes would falsify the claim of a unified underlying mechanism.

**Preregistration:** All primary endpoints, inference procedures, stopping rules, and exclusion criteria must be preregistered (e.g., via OSF/AsPredicted). Any deviations from the pre-registered plan must be explicitly documented and justified.

**Data and Code Availability:** All analysis scripts (including power and sample-size simulations), anonymized raw data, experimental logs, and time-stamps (with random seeds where applicable) will be made publicly available at a persistent repository to ensure full transparency and reproducibility. The preregistration will link directly to this repository.

